---
layout: post
title:  "분류기 - 로지스틱 회귀, 나이브 베이즈"
date:   2017-09-13 17:10:13 +0800
categories: machinelearning
tags: MLwithPython
comments: 1
---
로지스틱 회귀 분류기와 나이브 베이즈 분류기 

---

##### 로지스틱 회귀 분류기

로지스틱 회귀 분석이란 입력 변수와 출력 변수의 관계를 표현헌ㄴ 기법 중 하나다. 여기서 입력은 독립 변수고 출력은 종속 변수다. 분류 문제를 다룰 때는 종속 변수로 분류할 클래스를 표현한다.
로지스틱 회귀 분석은 종속, 독립 변수를 로지스틱 함수를 통해 계산된 확률로 표현한다. 이때 로지스틱 함수는 시그모이드 곡선으로 표현한다. 이 곡선은 생명 주기를 표현할 때 자주 보던 S자를 눕힌 모양의 곡선으로 여러가지 매개변수로 구성된 함수를 만들 때 주로 이용한다. 로지스틱은 데이터의 분포를 표현하는 직선 중에 오차가 가장 적은 직선을 구하는 일반 선형 모델과 밀접한 관계가 있다. 이 절에서는 선형 회귀 분석 대신 로지스틱으로 대신한다. 엄밀히 말하면 로지스틱은 분류 기법은 아니지만 분류 문제에서 효과적이고 간결하기 때문에 머신 러닝에서 굉장히 많이 이용 된다.


{% highlight python linenos %}
import numpy as np
from sklearn import linear_model
import matplotlib.pyplot as plt

#from utilities import visualize_classifier #나중에 utilities.py 로 분류한다.

{% endhighlight %}


{% highlight python linenos %}


def visualize_classifier( classifier, X, y):
    min_x, max_x = X[:, 0].min() - 1.0, X[:, 0].max() + 1.0
    min_y, max_y = X[:, 1].min() - 1.0, X[:, 1].max() + 1.0
    mesh_step_size = 0.01

    x_vals, y_vals = np.meshgrid(np.arange(min_x, max_x, mesh_step_size), np.arange(min_y, max_y, mesh_step_size))
    
    # 메시그리드에 대해 분류기 실행
    output = classifier.predict(np.c_[x_vals.ravel(), y_vals.ravel()])
    
    #결과로 나온 배열정리
    output = output.reshape(x_vals.shape)
    
    #그래프 생성
    plt.figure()
    
    # 그래프 색상 지정
    plt.pcolormesh(x_vals, y_vals, output, cmap=plt.cm.gray)
    
    # 그래프에 학습용 점 그리기
    plt.scatter(X[:, 0], X[:, 1], c=y, s=75, edgecolors='black', linewidth=1, cmap=plt.cm.Paired )

    plt.xlim(x_vals.min(), x_vals.max())
    plt.ylim(y_vals.min(), y_vals.max())
    
    # x, y 축 눈금 표시
    plt.xticks((np.arange(int(X[:, 0].min() -1), int(X[:, 0].max() + 1), 1.0)))
    plt.yticks((np.arange(int(X[:, 1].min() -1), int(X[:, 1].max() + 1), 1.0)))
    
    plt.show()
    
# 샘플 데이터 정의
X = np.array([[3.1, 7.2],[4, 6.7], [2.9, 8], [5.1, 4.6], [6, 5], [5.6, 5],
              [3.3, 0.4], [3.9, 0.9], [2.8, 1], [0.5, 3.4], [1, 4], [0.6, 4.9]])
y = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3])

# 로지스틱 회귀 분류기 생성
classifier = linear_model.LogisticRegression(solver='liblinear', C=1)

# 분류기 학습
classifier.fit(X, y)


# 분류기 성능 시각화
visualize_classifier(classifier, X, y)


# Accuracy of Naive Bayes classifier = 99.75 %

{% endhighlight %}


![img]({{baseurl}}/assets/res/machinelearning/ai-2-1.png)


##### 나이브 베이즈 분류기
나이브 베이즈 분류 기법은 베이즈 정리를 기반으로 분류기를 만든다. 베이즈 정리는 사건이 발생할 확률을 그 사건에 관련된 여러 가지 조건을 기반으로 표현된다. 문제의 인스턴스를 클래스 레이블에 할당하는 방식으로 이때 주어진 특징 값들은 서로 독립적이라 가정한다. 이를 독립성 가정이라 부르며 이름에 나이브라는 순진한 의미가 붙은 이유가 여기 있다.

서로 다른 특징들 사이에 관계는 고려치 않고 주어진 클래스 변수에 개별 특징이 미치는 효과만 본다.


{% highlight python linenos  %}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.naive_bayes import GaussianNB
from sklearn import cross_validation
{% endhighlight %}

{% highlight ruby %}
    
# 데이터를 담을 입력 파일
input_file = 'data_multivar_nb.txt'

# 입력파일에서 데이터 가져오기
data = np.loadtxt(input_file, delimiter=',')
X, y = data[:, :-1], data[:, -1]

classifier = GaussianNB()

# 분류기 학습하기
classifier.fit(X, y)

# 학습한 분류기로 예측한 결과 구하기
y_pred = classifier.predict(X)


# 정확도 계산
accuracy = 100.0 * (y == y_pred).sum() / X.shape[0]
print(round(accuracy,2))
print("Accuracy of Naive Bayes classifier =", round(accuracy, 2), "%")

# 분류기 성능 시각화
visualize_classifier(classifier, X, y)

# 데이터를 학습용과 테스트용으로 나누기
X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.2, random_state=3)
classifier_new = GaussianNB()
classifier_new.fit(X_train, y_train)
y_test_pred = classifier_new.predict(X_test)


# 분류기 정확도 계산
accuracy = 100.0 * (y_test == y_test_pred).sum() / X_test.shape[0]
print("Accuracy of new classifier =", round(accuracy, 2), "%")

# 분류기 성능 시각화
visualize_classifier(classifier_new, X_test, y_test)

num_folds = 3

accuracy_values = cross_validation.cross_val_score(classifier, X, y, scoring='accuracy', cv=num_folds)
print("Accuracy: "+ str(round(100*accuracy_values.mean(), 2)) + "%")
precision_values = cross_validation.cross_val_score(classifier, X, y, scoring='precision_weighted', cv=num_folds)
print("Precision : " + str(round(100*precision_values.mean(), 2)) + "%")

recall_values = cross_validation.cross_val_score(classifier, X, y, scoring='recall_weighted', cv=num_folds)
print("Recall : " + str(round(100*recall_values.mean(), 2)) + "%")
f1_values = cross_validation.cross_val_score(classifier, X, y, scoring='f1_weighted', cv=num_folds)
print("F1 : " + str(round(100*recall_values.mean(), 2)) + "%")

# Accuracy: 99.75%
# Precision : 99.76%
# Recall : 99.75%
# F1 : 99.75%

{% endhighlight %}


![img]({{baseurl}}/assets/res/machinelearning/ai-2-2.png)

![img]({{baseurl}}/assets/res/machinelearning/ai-2-3.png)



##### 참고자료

[조대협님 블로그][Naive ref-1]

[코세라 NLP 강의][Naive ref-2]


[Naive ref-1]: http://bcho.tistory.com/1010
[Naive ref-2]: https://www.coursera.org/learn/natural-language-processing
